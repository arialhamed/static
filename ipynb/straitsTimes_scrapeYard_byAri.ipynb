{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ari's Scrapeyard: Straits Times\n",
        "The Straits Times is one of Singapore's most popular newspapers. Due to its popularity & its need to stay relevant in this world ruled by tech, Straits Times has their own website they've been running for quite some time, [here it is](https://straitstimes.com). Using its [sitemap](https://straitstimes.com/sitemap.xml), I was able to scrape, well, a lot from it. \n",
        "\n",
        "let's take a look at the notes."
      ],
      "metadata": {
        "id": "EqLG28iGMW0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTu2PDniLp_t"
      },
      "outputs": [],
      "source": [
        "!pip install selenium tqdm\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!rm -rf sample_data\n",
        "\n",
        "# for computing total time taken, and also time taken for each model's training\n",
        "import time\n",
        "import pytz\n",
        "from datetime import timedelta, datetime\n",
        "time_alpha = time.time()\n",
        "\n",
        "# for filestuffs, and some pretty printing\n",
        "import os\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# for data scraping\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# data scraping webdriver\n",
        "from selenium import webdriver\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "# driver.maximize_window()\n",
        "\n",
        "# basic data manipulation & model training libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample \n",
        "from typing import List, Tuple # for types\n",
        "# from keras import layers\n",
        "# from keras.models import Model\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "# lastly, this is for visualization\n",
        "from keras.callbacks import TensorBoard\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saves URL & datetime of each ST article"
      ],
      "metadata": {
        "id": "ww5AgL9CMPNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# go british (init the dataframe)\n",
        "df = pd.DataFrame()\n",
        "\n",
        "save_url_only = True\n",
        "\n",
        "# from what i saw, today news & shinmin news also have sitemap, though I\n",
        "# would have to absolutely change the scraping methods here\n",
        "target_site = \"https://www.straitstimes.com/\"\n",
        "\n",
        "# get how much pages are there in straitstimes.com/sitemap.xml\n",
        "total_sitemap_pages = len([x for x in BeautifulSoup(urlopen(f\"{target_site}sitemap.xml\"),'lxml').get_text().split('\\n') if \"page=\" in x])\n",
        "\n",
        "# save file name\n",
        "full_raw_name = \"/content/straitstimes_sitemap.xml_full-raw.csv\"\n",
        "\n",
        "if not(os.path.isfile(full_raw_name)):\n",
        "  for i in tqdm(range(total_sitemap_pages)):\n",
        "    # assert i == 0 # for debugging\n",
        "\n",
        "    soup_text = BeautifulSoup(urlopen(f\"{target_site}sitemap.xml?page={str(i+1)}\"), 'lxml').get_text()\n",
        "    soup_url = [x for x in soup_text.split(\"\\n\") if \"https://\" in x and len(x) >= len(target_site)+1] # list of urls\n",
        "    soup_datetime = [x for x in soup_text.split(\"\\n\") if \"+08:00\" in x] # list of datetimes\n",
        "    \n",
        "    if save_url_only:\n",
        "      df = pd.concat([df, pd.DataFrame(\n",
        "          {\"url\": soup_url, \"datetime\": soup_datetime}\n",
        "      )], ignore_index=True)\n",
        "    else:\n",
        "      # Taking the each article text (bulk of time taken here)\n",
        "      soup_article = []\n",
        "      for url in tqdm(soup_url, leave=False):\n",
        "        os.system(f\"wget {url}\")\n",
        "        with open(url.split(\"/\")[-1], 'r') as f:        \n",
        "          soup_article.append('\\n'.join([y[3:-4] for y in [x.strip() for x in f.read().split(\"\\n\")] if \"<p>\" == y[:3] and \"</p>\" == y[-4:] and \" \" != y[3]]))\n",
        "        os.system(\"rm \"+url.split(\"/\")[-1])\n",
        "      \n",
        "      df = pd.concat([df, pd.DataFrame(\n",
        "          {\"url\": soup_url, \"datetime\": soup_datetime, \"article\": soup_article}\n",
        "      )], ignore_index=True)\n",
        "    \n",
        "  df.drop_duplicates(inplace=True)\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  # backup to runtime if error in notebook occurs\n",
        "  df.to_csv(full_raw_name)\n",
        "\n",
        "  \n",
        "else:\n",
        "  print(\"Already got the data, proceeding with it..\")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "vtw37WNoMHz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you're running this on colab i'd highly recommend creating someway to save the outputs if this notebook finishes execution when you're away. the total estimated time taken to go through all 35 pages is at least **50 hours**. google colab does not allow that, so either way i'd would recommend this notebook to be executed on your own machine in the end."
      ],
      "metadata": {
        "id": "H6AHEgYhrvD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note to ari: work on a way for waypoints, so that you can run and stop this \n",
        "# at any point of time."
      ],
      "metadata": {
        "id": "hFihhFrYrunq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}