{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# General Images Ripper\n",
        "Salut! This notebook has the power to _rip images from almost anywhere in the web!_ Its kinda like `ripme.jar` but much more customizable and doesn't require to be updated once in a while (this notebook's up to me update it, unfortunately lol). This is a **_work in progress_**, and if you want this notebook to rip a specific site that you want, then you gotta figure that out yourself, unless you want to pay me then go to [contact me](https://arifhamed.com/contact). The functions here should suffice.\n",
        "\n",
        "Requirements: \n",
        "- **Internet** connection\n",
        "- **Google Account** \n",
        "- A **browser** to run this\n",
        "- (Optional) **`curl`** (If you are running on Linux then it should be no problem). One of the download methods makes use of https://file.io, a command-line file transfer site. Very similar in function like https://wetransfer.com, but instead of email its for command-line.\n",
        "\n",
        "Usage: \n",
        "- Run the first cell (mandatory)\n",
        "- Run whatever cell that is labelled by its respective markdown block above it. \n",
        "- Sit back & relax. You can do other things while this notebook will do its magic. Google Colab will send a notification after its done (if you allowed it to do so through the browser)\n",
        "- Once its done, the following would occur if your `dl_mode` in the `save_content()` function is set as:\n",
        "  - `curl`\n",
        "    - The `curl` command will be shown as a string in the output of the cell that has `dl_mode` set as `curl` in the `save_content()` function. Just **copy & paste** the string to your **terminal/console** of choice (works on **Ubuntu 22.04**, **fastest option**). The output files are uploaded to https://file.io for temporary storage, which will last for 24 hours, and it will delete itself after 1 download. There's much more options for uploading, check it out on their website.\n",
        "  - `files` \n",
        "    - The output files will be normally downloaded through the browser (slow, but reliable if https://file.io becomes unstable or if Google Drive gets hacked)\n",
        "  - `drive` \n",
        "    - This notebook may require you to mount your [**Google Drive**](https://drive.google.com) to this instance so that it can move the output files to your Google Drive instead. This is good if you are not on a _machine that has much space_ and if you have a _lot of space to spare in your drive_. Recommended if you either use **ChromeOS**, if you **don't immediately have your main machine with you**, or if you're very **cloud-centric**. "
      ],
      "metadata": {
        "id": "Gp6lHCQGo6_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install img2pdf\n",
        "\n",
        "import time, pytz, os, sys, requests, codecs, json\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import timedelta, datetime\n",
        "from IPython.display import clear_output \n",
        "time_alpha = time.time()\n",
        "\n",
        "# Essential to make sure webpages does not detect you as a bot (even though this\n",
        "# is basically a bot). Does not work for those that require Javascript (WIP for \n",
        "# a workaround)\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "def request_with_headers(url, sep=\"\\n\"): # Returns list\n",
        "  return requests.get(url, headers=headers).content.decode().split(sep)\n",
        "\n",
        "file_io_command = \"\" # For curl\n",
        "\n",
        "def save_content(savename=\"output\", type_mode=\"zip\", dl_mode=\"files\", pdf_args=\"\"): \n",
        "  global file_io_command\n",
        "  # clear_output() # Clears previous file_io_command if there was\n",
        "  os.system(f\"mkdir -p /content/{type_mode}\")\n",
        "  savename = f\"/content/{type_mode}/{savename}.{type_mode}\"\n",
        "  if type_mode == \"zip\":\n",
        "    os.system(f\"zip -r {savename} /content/*.*\")\n",
        "  elif type_mode == \"pdf\":\n",
        "    os.system(f\"img2pdf /content/*.* -o {savename} --producer=\\\"Google Colab\\\"  {pdf_args}\")\n",
        "  \n",
        "  # I really wanted to use `match` (released in Python 3.10) here \n",
        "  # but Google Colab only has Python 3.8.16 as of this typing\n",
        "  if dl_mode == \"files\":\n",
        "    files.download(savename)\n",
        "  if dl_mode == \"curl\":\n",
        "    obj_temp = json.loads(os.popen(f'curl -F \"file=@{savename}\" https://file.io').read())\n",
        "    file_io_command += f\"curl {obj_temp['link']} --output {obj_temp['name']}; \"\n",
        "  if dl_mode == \"drive\":\n",
        "    os.system(f\"mv {savename} /content/drive/MyDrive/{savename.split('/')[-1]}\")\n",
        "\n",
        "  # removes all if it is a file in /content\n",
        "  for i in [x for x in os.listdir(\"/content\") if os.path.isfile(x)]:\n",
        "    try: os.remove(i)\n",
        "    except: pass\n",
        "\n",
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "yzCv4q_UIRvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Only run this cell if you want to use your Google Drive"
      ],
      "metadata": {
        "id": "B0trKpFWIYA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir -p /drive\n",
        "!mount --bind /content/drive/My\\ Drive /drive\n",
        "!mkdir -p ~/.ssh"
      ],
      "metadata": {
        "id": "kMGhnG_BIN_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## junji-ito.com\n",
        "As of 15 Jan 2022, these 4 are publicly available on Junji Ito's site"
      ],
      "metadata": {
        "id": "S2MxBBIfVCFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the lists are set to variables so that tqdm knows how to much there is inside \n",
        "# in each iteration. \n",
        "for comic in tqdm((\"tomie\", \"uzumaki\", \"gyo\", \"remina\")):\n",
        "  list_focus_i = [\"http\"+x.split(\"\\\"\")[0] for x in request_with_headers(f\"https://junji-ito.com/{comic}/\",sep=\"http\") if f\"{comic}-chapter\" in x][::-1]\n",
        "  # n is album index\n",
        "  for n, i in tqdm(enumerate(list_focus_i), total=len(list_focus_i), leave=False): \n",
        "    list_focus_j = list(dict.fromkeys([x.split(\"\\\"\")[0] for x in request_with_headers(i, sep='property=\"og:image\" content=\"')][1:]))\n",
        "    for j in tqdm(list_focus_j, total=len(list_focus_j), leave=False):\n",
        "      os.system(f\"wget {j} -O {str(n).zfill(2)}_{j.split('/')[-1]}\")\n",
        "  save_content(f\"junji-ito_{comic}\", type_mode=\"pdf\", dl_mode=\"curl\", pdf_args=f\" --title=\\\"{comic.capitalize()}\\\" --author=\\\"Junji Ito\\\" --subject=\\\"Horror\\\"\")\n",
        "\n",
        "# note for ari: reconsider this line below, cuz other people might want to use other forms of dl-ing\n",
        "file_io_command"
      ],
      "metadata": {
        "id": "6aF17H0qU_Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## readuzumaki.com\n",
        "this one has the lost chapter, and filesize is smaller"
      ],
      "metadata": {
        "id": "Of9TwLP73pU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(20)):\n",
        "  for j in [x.split(\"\\\"\")[0] for x in request_with_headers(f\"https://readuzumaki.com/manga/uzumaki-chapter-{i+1}/\", sep=\"<meta property=\\\"twitter:image\\\" content=\\\"\") if x[:4] == \"http\"]:\n",
        "    os.system(f\"wget -O {str(i).zfill(2)}_{j.split('/')[-1]} {j}\")\n",
        "\n",
        "save_content(\"readuzumaki\", type_mode=\"pdf\", dl_mode=\"curl\", pdf_args=f\" --title=\\\"Uzumaki\\\" --author=\\\"Junji Ito\\\"\")\n",
        "file_io_command"
      ],
      "metadata": {
        "id": "yEDgu53S3ksh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carousell search (first page)\n"
      ],
      "metadata": {
        "id": "zo_0fHFV3lO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace space with \"+\"\n",
        "query = \"thinkpad%2025\"\n",
        "\n",
        "# result = [y for y in [codecs.decode(x.split(\"\\\"\")[0], \"unicode-escape\") for x in request_with_headers(f\"https://www.carousell.sg/search/{query}\", '\":\"') if x[:4] == \"http\"] if y[-4:] in (\".png\",\".jpg\",\"jpeg\",\"webp\",\".gif\",\".mp4\") and not(y.split(\"/\")[-1][0].isnumeric())]\n",
        "# result = [\"http\"+codecs.decode(x.split(\"\\\"\")[0], \"unicode-escape\") for x in requests.get(f\"https://www.carousell.sg/search/{query}\", headers=headers).content.decode().split('http') if x[:20] == \"s://www.carousell.sg\"]\n",
        "result = [codecs.decode(x.split(\"\\\"\")[0], \"unicode-escape\") for x in request_with_headers(f\"https://www.carousell.sg/search/{query}?addRecent=true&canChangeKeyword=true&includeSuggestions=true\", 'http') if \"anniversary\" in x]\n",
        "\n",
        "\n",
        "for i in result: \n",
        "  print(repr(i))\n",
        "  # if not(os.path.isfile(i.split(\"/\")[-1])):\n",
        "  #   print(i)\n",
        "  #   os.system(f\"wget {i}\")\n",
        "\n",
        "# save_content(f\"carousell_{query}.zip\")"
      ],
      "metadata": {
        "id": "hijzJvo4IZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if drive is mounted, it is 1\n",
        "if_drive_mounted = int(os.popen('[ -d \"/content/drive/\" ] && echo 1 || echo 0').read()[:-1])\n",
        "if if_drive_mounted:\n",
        "  print('bruh')\n",
        "else:\n",
        "  print('cunt')\n",
        "if_drive_mounted"
      ],
      "metadata": {
        "id": "pXFxq60BCm3n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}