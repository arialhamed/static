{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8ZArBVT3Huyi",
        "zo_0fHFV3lO0",
        "Jhyb7d3csWGj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# General Images Ripper\n",
        "Salut! This notebook has the power to _rip images from almost anywhere in the web!_ Its kinda like `ripme.jar` but much more customizable and doesn't require to be updated once in a while (this notebook's up to me update it, unfortunately lol). This is a **_work in progress_**, and if you want this notebook to rip a specific site that you want, then you gotta figure that out yourself, unless you want to pay me then go to [contact me](https://arifhamed.com/contact). The functions here should suffice.\n",
        "\n",
        "Requirements: \n",
        "- **Internet** connection\n",
        "- **Google Account** \n",
        "- A **browser** to run this\n",
        "- (Optional) **`curl`** (If you are running on Linux then it should be no problem). One of the download methods makes use of https://file.io, a command-line file transfer site. Very similar in function like https://wetransfer.com, but instead of email its for command-line.\n",
        "\n",
        "Usage: \n",
        "- Run the first cell (mandatory)\n",
        "- Run whatever cell that is labelled by its respective markdown block above it. \n",
        "- Sit back & relax. You can do other things while this notebook will do its magic. Google Colab will send a notification after its done (if you allowed it to do so through the browser)\n",
        "- Once its done, the following would occur if your `dl_mode` in the `save_content()` function is set as:\n",
        "  - `curl`\n",
        "    - The `curl` command will be shown as a string in the output of the cell that has `dl_mode` set as `curl` in the `save_content()` function. Just **copy & paste** the string to your **terminal/console** of choice (works on **Ubuntu 22.04**, **fastest option**). The output files are uploaded to https://file.io for temporary storage, which will last for 24 hours, and it will delete itself after 1 download. There's much more options for uploading, check it out on their website.\n",
        "  - `files` \n",
        "    - The output files will be normally downloaded through the browser (slow, but reliable if https://file.io becomes unstable or if Google Drive gets hacked)\n",
        "  - `drive` \n",
        "    - This notebook may require you to mount your [**Google Drive**](https://drive.google.com) to this instance so that it can move the output files to your Google Drive instead. This is good if you are not on a _machine that has much space_ and if you have a _lot of space to spare in your drive_. Recommended if you either use **ChromeOS**, if you **don't immediately have your main machine with you**, or if you're very **cloud-centric**. "
      ],
      "metadata": {
        "id": "Gp6lHCQGo6_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install img2pdf \n",
        "\n",
        "import time, pytz, os, sys, requests, codecs, json\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import timedelta, datetime\n",
        "from IPython.display import clear_output \n",
        "time_alpha = time.time()\n",
        "\n",
        "# Essential to make sure webpages does not detect you as a bot (even though this\n",
        "# is basically a bot). Does not work for those that require Javascript (WIP for \n",
        "# a workaround)\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "def request_with_headers(url, sep=\"\\n\"): # Returns list\n",
        "  return requests.get(url, headers=headers).content.decode().split(sep)\n",
        "\n",
        "file_io_command = \"\" # For curl\n",
        "\n",
        "def save_content(savename=\"output\", type_mode=\"zip\", dl_mode=\"files\", pdf_args=\"\"): \n",
        "  global file_io_command\n",
        "  # clear_output() # Clears previous file_io_command if there was\n",
        "  os.system(f\"mkdir -p /content/{type_mode}\")\n",
        "  savename = f\"/content/{type_mode}/{savename}.{type_mode}\"\n",
        "  if type_mode == \"zip\":\n",
        "    os.system(f\"zip -r {savename} /content/*.*\")\n",
        "  elif type_mode == \"pdf\":\n",
        "    os.system(f\"img2pdf /content/*.* -o {savename} --producer=\\\"Google Colab\\\"  {pdf_args}\")\n",
        "  \n",
        "  # I really wanted to use `match` (released in Python 3.10) here \n",
        "  # but Google Colab only has Python 3.8.16 as of this typing\n",
        "  if dl_mode == \"files\":\n",
        "    files.download(savename)\n",
        "  elif dl_mode == \"curl\":\n",
        "    temp_str = os.popen(f'curl -F \"file=@{savename}\" https://file.io').read()\n",
        "    # DEBUGGING print()\n",
        "    print(repr(temp_str)) \n",
        "    obj_temp = json.loads(temp_str)\n",
        "    file_io_command += f\"curl \\\"{obj_temp['link']}\\\" --output \\\"{obj_temp['name']}\\\"; \"\n",
        "  elif dl_mode == \"drive\":\n",
        "    os.system(f\"mv {savename} /content/drive/MyDrive/{savename.split('/')[-1]}\")\n",
        "  else: #dl_mode == \"\"\n",
        "    pass\n",
        "  \n",
        "  # removes all if it is a file in /content\n",
        "  for i in [x for x in os.listdir(\"/content\") if os.path.isfile(x)]:\n",
        "    try: os.remove(i)\n",
        "    except: pass\n",
        "\n",
        "# !rm -rf /content/*\n",
        "for i in (\"pdf\",\"zip\"):\n",
        "  os.system(f\"rm -rf /content/*.* /content/{i}/\")"
      ],
      "metadata": {
        "id": "yzCv4q_UIRvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Only run this cell if you want to use your Google Drive"
      ],
      "metadata": {
        "id": "B0trKpFWIYA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir -p /drive\n",
        "!mount --bind /content/drive/My\\ Drive /drive\n",
        "!mkdir -p ~/.ssh"
      ],
      "metadata": {
        "id": "kMGhnG_BIN_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## junji-ito.com\n",
        "As of 15 Jan 2022, these 4 are publicly available on Junji Ito's site"
      ],
      "metadata": {
        "id": "S2MxBBIfVCFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the lists are set to variables so that tqdm knows how to much there is inside \n",
        "# in each iteration. \n",
        "for comic in tqdm((\"tomie\", \"uzumaki\", \"gyo\", \"remina\")):\n",
        "  list_focus_i = [\"http\"+x.split(\"\\\"\")[0] for x in request_with_headers(f\"https://junji-ito.com/{comic}/\",sep=\"http\") if f\"{comic}-chapter\" in x][::-1]\n",
        "  # n is album index\n",
        "  for n, i in tqdm(enumerate(list_focus_i), total=len(list_focus_i), leave=False): \n",
        "    list_focus_j = list(dict.fromkeys([x.split(\"\\\"\")[0] for x in request_with_headers(i, sep='property=\"og:image\" content=\"')][1:]))\n",
        "    for j in tqdm(list_focus_j, total=len(list_focus_j), leave=False):\n",
        "      os.system(f\"wget {j} -O {str(n).zfill(2)}_{j.split('/')[-1]}\")\n",
        "  save_content(f\"junji-ito_{comic}\", type_mode=\"pdf\", dl_mode=\"curl\", pdf_args=f\" --title=\\\"{comic.capitalize()}\\\" --author=\\\"Junji Ito\\\" --subject=\\\"Horror\\\"\")\n",
        "\n",
        "# note for ari: reconsider this line below, cuz other people might want to use other forms of dl-ing\n",
        "file_io_command"
      ],
      "metadata": {
        "id": "6aF17H0qU_Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## readuzumaki.com\n",
        "this one has the lost chapter, and filesize is smaller"
      ],
      "metadata": {
        "id": "Of9TwLP73pU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(20)):\n",
        "  for j in [x.split(\"\\\"\")[0] for x in request_with_headers(f\"https://readuzumaki.com/manga/uzumaki-chapter-{i+1}/\", sep=\"<meta property=\\\"twitter:image\\\" content=\\\"\") if x[:4] == \"http\"]:\n",
        "    os.system(f\"wget -O {str(i).zfill(2)}_{j.split('/')[-1]} {j}\")\n",
        "\n",
        "save_content(\"readuzumaki\", type_mode=\"pdf\", dl_mode=\"curl\", pdf_args=f\" --title=\\\"Uzumaki\\\" --author=\\\"Junji Ito\\\"\")\n",
        "file_io_command"
      ],
      "metadata": {
        "id": "yEDgu53S3ksh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########################################################################################\n",
        "\n",
        "# Caution: below this point are all WIPs\n",
        "\n",
        "########################################################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "HmkyGupXPXgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Punch Man (manga, not the webcomic)\n",
        "URL: https://ww1.onepunch.online/"
      ],
      "metadata": {
        "id": "Ovnzf64vR4Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/*\n",
        "i_delimeter=\"https://ww1.onepunch.online/manga/one\"\n",
        "i_list = [i_delimeter+x.split(\"\\\"\")[0] for x in request_with_headers(f\"https://ww1.onepunch.online/\", sep=i_delimeter)[1:-3]][::-1]\n",
        "for n, i in tqdm(enumerate(i_list), total=len(i_list)):\n",
        "  # assert n < 3\n",
        "  j_delimeter = \"https://cdn.hxmanga.com/file/\"\n",
        "  j_list = list(dict.fromkeys([j_delimeter+x.split(\"\\\"\")[0] for x in request_with_headers(i, sep=j_delimeter)[1:-1]]))\n",
        "  \n",
        "  tempo = i.split('-chapter-')[-1][:-1]\n",
        "  tempo = str(tempo).zfill(3 + tempo.count(\"-\") * 2)\n",
        "\n",
        "  for j in tqdm(j_list, total=len(j_list), leave=False):\n",
        "    current_query = f\"wget {j} -O /content/{tempo}_{j.split('/')[-1].zfill(6)}\"\n",
        "    os.system(current_query)\n",
        "  \n",
        "  save_content(f\"one-punch-man_issue-{tempo}\", type_mode=\"pdf\", dl_mode=\"\", pdf_args=f\" --title=\\\"One Punch Man: Issue {str(i.split('-chapter-')[-1][:-1].split('-')[0]).zfill(3)}\\\" --creator=\\\"ONE\\\" --author=\\\"Yusuke Murata\\\" --subject=\\\"Action\\\"\")\n",
        "\n",
        "dl_mode = \"drive\"\n",
        "###################### file.io maximum usage is 2GB, so must split liao\n",
        "if dl_mode == \"curl\":\n",
        "  def split_dir(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
        "  current_size = 0\n",
        "  target_dir_str = \"/content/pdf/\"\n",
        "  target_dir = os.listdir(target_dir_str)\n",
        "  for i in os.listdir(target_dir_str):\n",
        "    current_size += os.path.getsize(target_dir_str+i)\n",
        "  # for i in range(int(str(current_size / 1800000000).split(\".\")[0])+1):\n",
        "\n",
        "  for n, i in enumerate(list(split_dir(target_dir, int(str(current_size / 1800000000).split(\".\")[0])+1))):\n",
        "    # print(len(i))\n",
        "    for j in i:\n",
        "      os.system(f\"mv {target_dir_str}{j} {j}\")\n",
        "    torget = f\"one-punch-{str(n).zfill(2)}.zip\"\n",
        "    os.system(f\"zip -r {torget} /content/*.pdf\")\n",
        "    temp_obj = json.loads(os.popen(f'curl -F \"file=@{torget}\" https://file.io').read())\n",
        "    # print(temp_obj)\n",
        "    file_io_command += f\"curl \\\"{temp_obj['link']}\\\" --output \\\"{temp_obj['name']}\\\"; \"\n",
        "    os.system(\"rm -rf /content/*.*\")\n",
        "if dl_mode == \"drive\":\n",
        "  torget = \"/content/one-punch.zip\"\n",
        "  os.system(f\"zip -r {torget} /content/pdf/\")\n",
        "  os.system(f\"mv {torget} /content/drive/MyDrive/{torget.split('/')[-1]}\")\n",
        "# temp_obj = json.loads(os.popen(f'curl -F \"file=@{torget}\" https://file.io').read())\n",
        "# print(temp_obj)\n",
        "# file_io_command = f\"curl \\\"{temp_obj['link']}\\\" --output \\\"{temp_obj['name']}\\\"; \"\n",
        "# file_io_command"
      ],
      "metadata": {
        "id": "MUwaqLhBR9eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit\n",
        "Original: [source](https://github.com/Watchful1/Sketchpad/blob/master/postDownloader.py)"
      ],
      "metadata": {
        "id": "8ZArBVT3Huyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit = '' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "I9cOJHNhHzc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "\n",
        "username = \"\"  # put the username you want to download in the quotes\n",
        "subreddit = \"thinkpad\"  # put the subreddit you want to download in the quotes\n",
        "thread_id = \"\"  # put the id of the thread you want to download in the quotes, it's the first 5 to 7 character string of letters and numbers from the url, like 107xayi\n",
        "# leave either one blank to download an entire user's or subreddit's history\n",
        "# or fill in both to download a specific users history from a specific subreddit\n",
        "\n",
        "convert_to_ascii = False  # don't touch this unless you know what you're doing\n",
        "\n",
        "filter_string = None\n",
        "if username == \"\" and subreddit == \"\" and thread_id == \"\":\n",
        "\tprint(\"Fill in username, subreddit or thread id\")\n",
        "\tsys.exit(0)\n",
        "\n",
        "filters = []\n",
        "if username:\n",
        "\tfilters.append(f\"author={username}\")\n",
        "if subreddit:\n",
        "\tfilters.append(f\"subreddit={subreddit}\")\n",
        "if thread_id:\n",
        "\tfilters.append(f\"link_id=t3_{thread_id}\")\n",
        "filter_string = '&'.join(filters)\n",
        "\n",
        "url = \"https://api.pushshift.io/reddit/{}/search?limit=1000&order=desc&{}&before=\"\n",
        "\n",
        "start_time = datetime.utcnow()  #datetime.strptime(\"10/05/2021\", \"%m/%d/%Y\")\n",
        "end_time = None  #datetime.strptime(\"09/25/2021\", \"%m/%d/%Y\")#datetime.utcnow()\n",
        "\n",
        "\n",
        "def downloadFromUrl(filename, object_type):\n",
        "\tprint(f\"Saving {object_type}s to {filename}\")\n",
        "\n",
        "\tcount = 0\n",
        "\tif convert_to_ascii:\n",
        "\t\thandle = open(filename, 'w', encoding='ascii')\n",
        "\telse:\n",
        "\t\thandle = open(filename, 'w', encoding='UTF-8')\n",
        "\tprevious_epoch = int(start_time.timestamp())\n",
        "\tbreak_out = False\n",
        "\twhile True:\n",
        "\t\tnew_url = url.format(object_type, filter_string)+str(previous_epoch)\n",
        "\t\tjson_text = requests.get(new_url, headers={'User-Agent': \"Post downloader by /u/Watchful1\"})\n",
        "\t\ttime.sleep(1)  # pushshift has a rate limit, if we send requests too fast it will start returning error messages\n",
        "\t\ttry:\n",
        "\t\t\tjson_data = json_text.json()\n",
        "\t\texcept json.decoder.JSONDecodeError:\n",
        "\t\t\ttime.sleep(1)\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tif 'data' not in json_data:\n",
        "\t\t\tbreak\n",
        "\t\tobjects = json_data['data']\n",
        "\t\tif len(objects) == 0:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tfor object in objects:\n",
        "\t\t\tprevious_epoch = object['created_utc'] - 1\n",
        "\t\t\tif end_time is not None and datetime.utcfromtimestamp(previous_epoch) < end_time:\n",
        "\t\t\t\tbreak_out = True\n",
        "\t\t\t\tbreak\n",
        "\t\t\tcount += 1\n",
        "\t\t\tif object_type == 'comment':\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\thandle.write(str(object['score']))\n",
        "\t\t\t\t\thandle.write(\" : \")\n",
        "\t\t\t\t\thandle.write(datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"))\n",
        "\t\t\t\t\thandle.write(\" : u/\")\n",
        "\t\t\t\t\thandle.write(object['author'])\n",
        "\t\t\t\t\thandle.write(\" : \")\n",
        "\t\t\t\t\thandle.write(f\"https://www.reddit.com{object['permalink']}\")\n",
        "\t\t\t\t\thandle.write(\"\\n\")\n",
        "\t\t\t\t\tif convert_to_ascii:\n",
        "\t\t\t\t\t\thandle.write(object['body'].encode(encoding='ascii', errors='ignore').decode())\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\thandle.write(object['body'])\n",
        "\t\t\t\t\thandle.write(\"\\n-------------------------------\\n\")\n",
        "\t\t\t\texcept Exception as err:\n",
        "\t\t\t\t\tprint(f\"Couldn't print comment: https://www.reddit.com{object['permalink']}\")\n",
        "\t\t\t\t\tprint(traceback.format_exc())\n",
        "\t\t\telif object_type == 'submission':\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\thandle.write(str(object['score']))\n",
        "\t\t\t\t\thandle.write(\" : \")\n",
        "\t\t\t\t\thandle.write(datetime.fromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\"))\n",
        "\t\t\t\t\thandle.write(\" : \")\n",
        "\t\t\t\t\tif convert_to_ascii:\n",
        "\t\t\t\t\t\thandle.write(object['title'].encode(encoding='ascii', errors='ignore').decode())\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\thandle.write(object['title'])\n",
        "\t\t\t\t\thandle.write(\" : u/\")\n",
        "\t\t\t\t\thandle.write(object['author'])\n",
        "\t\t\t\t\thandle.write(\" : \")\n",
        "\t\t\t\t\thandle.write(f\"https://www.reddit.com{object['permalink']}\")\n",
        "\t\t\t\t\thandle.write(\"\\n\")\n",
        "\t\t\t\t\tif object['is_self']:\n",
        "\t\t\t\t\t\tif 'selftext' in object:\n",
        "\t\t\t\t\t\t\tif convert_to_ascii:\n",
        "\t\t\t\t\t\t\t\thandle.write(object['selftext'].encode(encoding='ascii', errors='ignore').decode())\n",
        "\t\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\t\thandle.write(object['selftext'])\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\thandle.write(object['url'])\n",
        "\n",
        "\t\t\t\t\thandle.write(\"\\n-------------------------------\\n\")\n",
        "\t\t\t\texcept Exception as err:\n",
        "\t\t\t\t\tprint(f\"Couldn't print post: {object['url']}\")\n",
        "\t\t\t\t\tprint(traceback.format_exc())\n",
        "\n",
        "\t\tif break_out:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tprint(\"Saved {} {}s through {}\".format(count, object_type, datetime.fromtimestamp(previous_epoch).strftime(\"%Y-%m-%d\")))\n",
        "\n",
        "\tprint(f\"Saved {count} {object_type}s\")\n",
        "\thandle.close()\n",
        "\n",
        "if not thread_id:\n",
        "\tdownloadFromUrl(\"posts.txt\", \"submission\")\n",
        "# downloadFromUrl(\"comments.txt\", \"comment\")"
      ],
      "metadata": {
        "id": "cw_PUUneLhMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in [x[] for x in json.loads(request_with_headers(f\"https://www.reddit.com/r/thinkpad.json\")[0])['data']['children']]:\n",
        "for i in [\"https://i.redd.it\"+x for x in request_with_headers(f\"https://www.reddit.com/r/thinkpad.json?count=1000\",sep=\"redd.it\")]:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "4_d6w7_FHwq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carousell search (first page)\n"
      ],
      "metadata": {
        "id": "zo_0fHFV3lO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace space with \"+\"\n",
        "query = \"thinkpad%2025\"\n",
        "\n",
        "result = [codecs.decode(x.split(\"\\\"\")[0], \"unicode-escape\") for x in request_with_headers(f\"https://www.carousell.sg/search/{query}?addRecent=true&canChangeKeyword=true&includeSuggestions=true\", 'http') if \"anniversary\" in x]\n",
        "\n",
        "\n",
        "for i in result: \n",
        "  print(repr(i))\n",
        "# save_content(f\"carousell_{query}.zip\")"
      ],
      "metadata": {
        "id": "hijzJvo4IZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Audio section!!????\n",
        "yeah. still wip."
      ],
      "metadata": {
        "id": "Jhyb7d3csWGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt upgrade -y\n",
        "!apt install yt-dlp"
      ],
      "metadata": {
        "id": "T3EnvIu93rSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/*\n",
        "# !apt install youtube-dl\n",
        "# !wget https://github.com/arifhamed/static/raw/main/others/revive_music.txt\n",
        "\n",
        "!youtube-dl -f \"bestaudio/best\" --extract-audio --audio-format flac --audio-quality 0 -a revive_music.txt\n",
        "# print(os.popen('youtube-dl -f \"bestaudio/best\" --extract-audio --audio-format flac --audio-quality 0 -a revive_music.txt').read())\n",
        "# save_content(\"music\",\"zip\",\"drive\")\n",
        "\n",
        "!mkdir -p /content/zip\n",
        "!mv /content/*.* /content/zip/\n",
        "\n",
        "def split_dir(a, n):\n",
        "  k, m = divmod(len(a), n)\n",
        "  return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
        "current_size = 0\n",
        "target_dir_str = \"/content/zip/\"\n",
        "target_dir = os.listdir(target_dir_str)\n",
        "for i in os.listdir(target_dir_str):\n",
        "  current_size += os.path.getsize(target_dir_str+i)\n",
        "# for i in range(int(str(current_size / 1800000000).split(\".\")[0])+1):\n",
        "\n",
        "for n, i in enumerate(list(split_dir(target_dir, int(str(current_size / 1800000000).split(\".\")[0])+1))):\n",
        "  # print(len(i))\n",
        "  for j in i:\n",
        "    os.system(f\"mv {target_dir_str}{j} {j}\")\n",
        "  torget = f\"music-{str(n).zfill(2)}.zip\"\n",
        "  os.system(f\"zip -r {torget} /content/*.pdf\")\n",
        "  temp_obj = json.loads(os.popen(f'curl -F \"file=@{torget}\" https://file.io').read())\n",
        "  # print(temp_obj)\n",
        "  file_io_command += f\"curl \\\"{temp_obj['link']}\\\" --output \\\"{temp_obj['name']}\\\"; \"\n",
        "  os.system(\"rm -rf /content/*.*\")\n",
        "\n",
        "file_io_command"
      ],
      "metadata": {
        "id": "d2gX_VaudIwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ps aux | grep youtube-dl\n",
        "!youtube-dl"
      ],
      "metadata": {
        "id": "9BNBhGy82NtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}