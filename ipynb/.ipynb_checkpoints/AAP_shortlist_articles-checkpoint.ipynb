{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8CYSZOG3jqHF",
   "metadata": {
    "id": "8CYSZOG3jqHF"
   },
   "source": [
    "\n",
    "```\n",
    "                                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n",
    "                                      ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó\n",
    "                                      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù\n",
    "                                      ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù\n",
    "                                      ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïë\n",
    "                                      ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïù\n",
    "```\n",
    "# A.I. APPLICATIONS PROJECT: PROJECT MILESTONE REPORT\n",
    "MODULE_GRP: IT3100-02<br>\n",
    "NRIC_NAME: MUHAMMAD ARIF BIN HAMED<br>\n",
    "ADMIN_NO: null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwp7FOKbSgVl",
   "metadata": {
    "id": "iwp7FOKbSgVl"
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"https://media1.giphy.com/media/L3Pp1dmjj8alAquswA/giphy.gif\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m2uSurPk-ebM",
   "metadata": {
    "id": "m2uSurPk-ebM"
   },
   "source": [
    "# <b>Classify [Straits Times](https://www.straitstimes.com/) Articles related to housing or not.</b>\n",
    "Each point can be found in certain cells, important cells (rubrics fulfillment) will have this emoji in it -> üë©üèæ‚Äçüíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqASVlVZqvi4",
   "metadata": {
    "id": "oqASVlVZqvi4"
   },
   "source": [
    "## Importing the libraries\n",
    "Only 1 library needs to pip installed for Colab: `sentencepiece`. More details on it when we get into subword tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WVkz9yl2qzdX",
   "metadata": {
    "id": "WVkz9yl2qzdX"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "\n",
    "# for computing total time taken, and also time taken for each model's training\n",
    "import time\n",
    "import pytz\n",
    "from datetime import timedelta \n",
    "from datetime import datetime\n",
    "time_alpha = time.time()\n",
    "\n",
    "# for filestuffs, and some pretty printing\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.backends.backend_pdf as mplpdf\n",
    "\n",
    "\n",
    "# for data scraping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# basic data manipulation & model training libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample \n",
    "from typing import List, Tuple # for types\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# lastly, this is for visualization\n",
    "from keras.callbacks import TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# prevent scientific notations\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6L8ue1LRMGLB",
   "metadata": {
    "id": "6L8ue1LRMGLB"
   },
   "source": [
    "## Scrape data directly from straitstimes.com/sitemap.xml\n",
    "Instead of scraping and storing CSV files online for this notebook to use, I was able to use `beautifulsoup` & `urllib.request` to scrape every single Straits Times article **_url_** and **_datetime_** ever since 2013. This may add more total time elapsed for using running this notebook, and there is a very small chance of your Google account getting locked out of Google Colab. **Use at your own risk!**\n",
    "\n",
    "As of 15 Dec 2022, it could take about ~6hr~ ~1hr~ ~30min~ **6min** to run this notebook. Some things here like the batch size and the number of epochs here can be changed, but that may lead to longer wait times. \n",
    "\n",
    "<br>\n",
    "\n",
    "## üë©üèæ‚Äçüíª Details about data collected from https://www.straitstimes.com/sitemap.xml\n",
    "Since my task involves (as of now only) classification of by using the **headlines** of a Straits Times article, data collection can be done on the whim by running the cell below. The Straits Times sitemap contains almost every known link that belongs the url https://www.straitstimes.com. \n",
    "\n",
    "Fortunately, unlike some other news sites like [Shin Min Daily](https://www.shinmin.sg/sitemap.xml), Straits Times uses a _slugified_ version of the headline of each article webpage as the webpage URL (Shin Min uses numbered date IDs)\n",
    "\n",
    "With this discovery, I was able to utilize Python's `beautifulsoup` & `urllib.request` libraries to extract virtually every single Straits Times article that has been uploaded to their website, spanning all the way from **1 Jan 2013** to **about a week ago**.\n",
    "\n",
    "Irregularities about the data & data preparation in the next markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IG1r5GPlMFYj",
   "metadata": {
    "id": "IG1r5GPlMFYj"
   },
   "outputs": [],
   "source": [
    "# go british (init the dataframe)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# get how much pages are there in straitstimes.com/sitemap.xml\n",
    "total_sitemap_pages = len([x for x in BeautifulSoup(urlopen(\"https://www.straitstimes.com/sitemap.xml\"),'lxml').get_text().split('\\n') if \"page=\" in x])\n",
    "\n",
    "full_raw_name = \"/content/straitstimes_sitemap.xml_full-raw.csv\"\n",
    "\n",
    "if not(os.path.isfile(full_raw_name)):\n",
    "  for i in range(total_sitemap_pages):\n",
    "    # assert i == 0 # for debugging\n",
    "    url = \"https://www.straitstimes.com/sitemap.xml?page=\"+str(i+1)\n",
    "    html = urlopen(url)\n",
    "\n",
    "    # note that using 'lxml' may not be available if you're running this \n",
    "    # notebook on a local runtime, which i would not recommend imo, as\n",
    "    # there could be a chance that straitstimes would time you out\n",
    "    soup = BeautifulSoup(html, 'lxml') \n",
    "    soup_as_text = soup.get_text()\n",
    "    soup_url = [x for x in soup_as_text.split(\"\\n\") if \"https://\" in x]\n",
    "    soup_datetime = [x for x in soup_as_text.split(\"\\n\") if \"+08:00\" in x]\n",
    "    \n",
    "    # first row in first page does not have datetime, hence remove url \n",
    "    # (which is just straitstimes.com)\n",
    "    if i == 0:\n",
    "      soup_url.remove(soup_url[0])\n",
    "      # in the first page, soup_datetime has the length of 4999, so soup_url will add up with soup_datetime\n",
    "\n",
    "    # appending to df\n",
    "    df = pd.concat([df, pd.DataFrame(\n",
    "        {\"url\": soup_url, \"datetime\": soup_datetime}\n",
    "    )], ignore_index=True)\n",
    "    \n",
    "    # pretty printing\n",
    "    if i == total_sitemap_pages - 1:\n",
    "      sys.stdout.write(\"\\rSaved all %i pages!\" % (i+1))\n",
    "    else:\n",
    "      sys.stdout.write(\"\\rSaving page %i\" % (i+1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "  # backup to runtime if error in notebook occurs\n",
    "  df.to_csv(full_raw_name)\n",
    "else:\n",
    "  print(\"Already got the data, proceeding..\")\n",
    "\n",
    "# as of 10 Dec 2022, this saved dataframe saves to a CSV that ends \n",
    "# up being around 23MB, so let's check it out here\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7YVkC-7upiD",
   "metadata": {
    "id": "e7YVkC-7upiD"
   },
   "source": [
    "## üë©üèæ‚Äçüíª Pre-process data\n",
    "Even though I have a vast size of **at least 170000 rows of data** (as of time of writing), there are quite a lot of irregularities with the data.\n",
    "* Drop cells containing NoneType & drop duplicate cells\n",
    "  * Due to how large the dataset is, Python could experience certain glitches when handling this amount of data. There are multiple instances in this code where there could suddenly be a _NoneType_ in an array of _String_. This will lead to a variety of errors along the way.\n",
    "  * The only way that I could counter this is by enforcing a check and drop rows that contain _NoneType_ right before a line that could experience errors involving sring could happen (like write text to a file or run text through a function, etc)\n",
    "  * This is not easy as I also have to drop other rows depending on the current task that the cell is undertaking. \n",
    "    * For example, if there is a _NoneType_ in texts list in index 2423, I must also drop label list in index 2423.\n",
    "* The `url` contains both the `category` and the `headline`, split them into **2 different columns** and then add it. The `category` is not used, just the `headline` column will be used as the **feature**\n",
    "* The `housing` column, which is the **target** column is based off the the `url` column containing certain keywords, such as _housing_, _hdb_, and a few others.\n",
    "* **IMPORTANT** BALANCING DATA:\n",
    "  * I realize after getting how much articles related to housing simple list comprehension showed, initially it was around **1000-3000** articles (depending on keyword lists), but i was using that against **174000-172000** non-housing articles. Even if I were generous with the keywords and get _5000_ articles, that is about **2.857% of the whole dataset**. The model will not be proficient at identifying housing articles against the ocean of ST articles.\n",
    "  * There are 2 methods to tackle this:\n",
    "    * _sklearn_'s resample function, which oversamples housing articles to raise it's percentage against the non-housing articles, or\n",
    "    * just **cut off a large portion of non-housing before splitting**. I did this because I don't want to risk overfitting the model based of the miniscule amount of housing articles that exist on Straits Times\n",
    "* Also do basic data cleaning.\n",
    "  * Especially for NLP, so I just switched the _dashes_ and _slashes_ with _spaces_ in `headline` column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bRBOfY0EuqHd",
   "metadata": {
    "id": "bRBOfY0EuqHd"
   },
   "outputs": [],
   "source": [
    "# this line is for debugging this cell, though it doesn't hurt to leave it here\n",
    "df = pd.read_csv('/content/straitstimes_sitemap.xml_full-raw.csv')\n",
    "\n",
    "full_prepared_name = \"/content/straitstimes_sitemap.xml_full-prepared.csv\"\n",
    "\n",
    "  # HOUSING DICTIONARY (more like array)\n",
    "housing_array = [\n",
    "  [ # First, mark all rows that has these\n",
    "    \"housing\",\n",
    "    # \"house\",\n",
    "    \"hdb\",\n",
    "    \"real-estate\",\n",
    "    \"private-estate\",\n",
    "    \"public-estate\",\n",
    "    \"home-approval\",\n",
    "    \"housing-estate\",\n",
    "    \"columbarium\",\n",
    "    # \"business\",\n",
    "    \"bto\"\n",
    "  ],\n",
    "  [ # This part of the array could be implemented, however for some reason it's not possible now\n",
    "    \"white-house\",\n",
    "    \"united-states\",\n",
    "    \"middle-east\",\n",
    "    \"israel\",\n",
    "    \"aye\",\n",
    "    \"interview-movie\",\n",
    "    \"syria\",\n",
    "    \"world/americas\",\n",
    "    \"entertainment\",\n",
    "    \"sph\",\n",
    "    \"mom\",\n",
    "    \"royal-infant\"\n",
    "  ]\n",
    "  ]\n",
    "\n",
    "# # This function is meant to be used in apply(lambda x: funct(x)) thing\n",
    "# # disabled for now\n",
    "# def label_housing(row):\n",
    "#   out_label = 0\n",
    "  \n",
    "#   for i in housing_array[0]:\n",
    "#     if i in row[\"url\"]:\n",
    "#       out_label = 1\n",
    "#   for i in housing_array[1]:\n",
    "#     if i in row[\"url\"]:\n",
    "#       out_label = 0\n",
    "#   return out_label\n",
    "\n",
    "if os.path.isfile(full_prepared_name):\n",
    "  df = pd.read_csv(full_prepared_name)\n",
    "  print(\"Prepared data exists, proceeding..\")\n",
    "else:\n",
    "  # drop duplicates & nonetypes, though there are likely no dupes\n",
    "  df.drop_duplicates(inplace=True)\n",
    "  df.dropna(inplace=True)\n",
    "\n",
    "  # remove the starting part of the url (\"https://www.straitstimes.com/\")\n",
    "  # split url into category and headline, \n",
    "  df[[\"category\",\"headline\"]] = df[\"url\"].str.slice(29,).str.rsplit('/',1,expand=True)\n",
    "\n",
    "  # add in the target column (simple filtering based of some keywords for now)\n",
    "  df['housing'] = df.apply(lambda row: 1 if  any(sbs in row['url'] for sbs in housing_array[0]) else 0, axis=1)\n",
    "\n",
    "  # replace slash & hyphens with space in category & headline respectively\n",
    "  # this is the only data cleaning needed as the url is already slugified\n",
    "  df[\"headline\"] = df[\"headline\"].str.replace(\"-\", \" \")\n",
    "\n",
    "  print(\"Current number of articles that are about housing: %i\\nDataset will be balanced to it.\" % sum(df[\"housing\"].tolist()))\n",
    "\n",
    "  # VERY IMPORTANT\n",
    "  # This part will cut off an amount of non housing articles\n",
    "  # this is decided by the ratio between housing & non-housing times a fixed no\n",
    "  # For example, at 3, 75% of the articles is now non-housing (4, 80%)\n",
    "  df_majority = df[df[\"housing\"]==0]                      # 175000 -> n * 3   \n",
    "  df_minority = df[df[\"housing\"]==1] # based              #      n \n",
    "  throwaway, df_majority = train_test_split(\n",
    "      df_majority, \n",
    "      test_size=len(df_minority.index)/len(df_majority.index)*3, \n",
    "      random_state=42, \n",
    "      shuffle=True\n",
    "  )\n",
    "  # combine both back again & randomize again, resetting index\n",
    "  df = pd.concat([df_majority, df_minority])\n",
    "  df = df.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "  ########## ##\n",
    "\n",
    "  # IMPORTANT: Force headlines to be string, else, cut them off\n",
    "  # iterations of this will unfortunately appear in later cells üò•\n",
    "  # IMPORTANT: THIS SHOULD BE FINAL BEFORE SPLITTING\n",
    "  temp_headline_for_null_removal = list(df[\"headline\"])\n",
    "  for i in range(len(temp_headline_for_null_removal)):\n",
    "    try: assert isinstance(temp_headline_for_null_removal[i],str)\n",
    "    except: df.drop(df.index[i])\n",
    "\n",
    "  print(\"Total df rows before real splitting: \"+str(len(df.index)))\n",
    "\n",
    "  # backup prepared data to hosted runtime \n",
    "  df.to_csv(full_prepared_name)\n",
    "\n",
    "# SPLIT\n",
    "df_train, df_test = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df[\"housing\"], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "df_train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QOR9UysYzrkF",
   "metadata": {
    "id": "QOR9UysYzrkF"
   },
   "source": [
    "## Preparing data for training and validation\n",
    "Further splitting here for training & validation. \n",
    "\n",
    "Random state was chosen as _42_. An in-depth explanation can be found in [this article](https://grsahagian.medium.com/what-is-random-state-42-d803402ee76b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jueq9Kr9usXl",
   "metadata": {
    "id": "jueq9Kr9usXl"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "val_split = 0.2\n",
    "\n",
    "df_train_copy = df_train.copy()\n",
    "texts = list(df_train_copy[\"headline\"]) # x\n",
    "labels = list(df_train_copy[\"housing\"]) # y\n",
    "\n",
    "# ensure that NOTHING IS NONETYPE. This could still happen after splitting with\n",
    "# keras_preprocessing's train_test_split, and i don't know why\n",
    "offaxis_removal = 0\n",
    "for i in range(len(texts)):\n",
    "  if not(isinstance(texts[i-offaxis_removal], str)):\n",
    "    del texts[i-offaxis_removal]\n",
    "    del labels[i-offaxis_removal]\n",
    "    offaxis_removal += 1 \n",
    "\n",
    "print('sample text: ', texts[0])\n",
    "print('corresponding label:', labels[0])\n",
    "\n",
    "# there should only be [1. 0.] & [0. 1.] representing 0 & 1 respectively\n",
    "# technically since this is a binary problem for now, i don't need to categorize\n",
    "# the labels, but just in case i need to use this notebook for further categorization\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# this function could have some problems (next comment)\n",
    "texts_train, texts_val, y_train, y_val = train_test_split(\n",
    "    texts, labels,\n",
    "    test_size=val_split,\n",
    "    random_state=random_state,\n",
    "    stratify=labels,\n",
    "    shuffle=True)\n",
    "\n",
    "## FURTHER EVICTING OF NONETYPES this is ridiculous\n",
    "# texts_train and y_train should be of same size, vice versa\n",
    "offaxis_removal = 0\n",
    "for i in range(len(texts_train)):\n",
    "  if not(isinstance(texts_train[i-offaxis_removal], str)):\n",
    "    del texts_train[i-offaxis_removal]\n",
    "    y_train = np.delete(y_train, i-offaxis_removal)\n",
    "    offaxis_removal += 1\n",
    "offaxis_removal = 0\n",
    "for i in range(len(texts_val)):\n",
    "  if not(isinstance(texts_val[i-offaxis_removal], str)):\n",
    "    del texts_val[i-offaxis_removal]\n",
    "    y_val = np.delete(y_val, i-offaxis_removal)\n",
    "    offaxis_removal += 1 \n",
    "\n",
    "print('labels shape:', labels.shape)\n",
    "print('train size: ', len(texts_train))\n",
    "print('validation size: ', len(texts_val))\n",
    "# If train size + validation size = labels shape[0], \n",
    "# then it's all set for training üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xN9m-SRo2U_D",
   "metadata": {
    "id": "xN9m-SRo2U_D"
   },
   "source": [
    "# Using 1D CNN Model for training\n",
    "\n",
    "In both subword-level and word-level, we will use the same CNN sequential model to compare which one would be more suitable for application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fUFSq9GP2Uoz",
   "metadata": {
    "id": "fUFSq9GP2Uoz"
   },
   "outputs": [],
   "source": [
    "def text_cnn(max_sequence_len: int, max_features: int, num_classes: int, \n",
    "              optimizer: str='adam', metrics: List[str]=['acc']) -> Model:\n",
    "    \n",
    "    sequence_input = layers.Input(shape=(max_sequence_len,), dtype='int32', name=\"Input\") # [(None, 16)]\n",
    "    embedded_sequences = layers.Embedding(\n",
    "        max_features, \n",
    "        256, \n",
    "        trainable=True, \n",
    "        name=\"Embedding\"\n",
    "    )(sequence_input)                                                                     # (None, 16, 256)\n",
    "    # LSTM could not be used due to some input compatibility problem, but its \n",
    "    # not so necessary as the size of each sample is very small\n",
    "    # lstm_embedded = layers.Bidirectional(layers.LSTM(128, return_sequences=True), name=\"BidirectionalLSTM\")(embedded_sequences)\n",
    "    conv1 = layers.Conv1D(128, 5, activation='relu', name=\"Conv1D_1\")(embedded_sequences) # (None, 12, 128)\n",
    "    pool1 = layers.MaxPooling1D(1, name=\"MaxPool1D_1\")(conv1)                             # (None, 12, 128)\n",
    "    conv2 = layers.Conv1D(128, 5, activation='relu', name=\"Conv1D_2\")(pool1)              # (None, 8, 128)\n",
    "    pool2 = layers.MaxPooling1D(2, name=\"MaxPool1D_2\")(conv2)                             # (None, 4, 128)\n",
    "    flatten = layers.Flatten(name=\"Flatten\")(pool2)                                       # (None, 512)\n",
    "    dens1 = layers.Dense(128, activation='relu', name=\"Dense_1\")(flatten)                 # (None, 128)\n",
    "    dens2 = layers.Dense(128, activation='relu', name=\"Dense_2\")(dens1)                   # (None, 128)\n",
    "    dens3 = layers.Dense(32, activation='relu', name=\"Dense_3\")(dens2)                    # (None, 32)\n",
    "    preds = layers.Dense(num_classes, activation='sigmoid', name=\"Dense_Preds\")(dens3)    # (None, 2)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    # binary crossentropy, cuz its yes/no if article is about housing\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fybjo8Pkotwl",
   "metadata": {
    "id": "fybjo8Pkotwl"
   },
   "source": [
    "## Prepare test data for possible use before training (such as callbacks)\n",
    "Testing dataset is already cleaned before splitting away from training dataset.\n",
    "\n",
    "Cell below prepares the data for testing\n",
    "\n",
    "This is just in case I want to make future improvements to the model creation process here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bm2fRJAjsU_n",
   "metadata": {
    "id": "Bm2fRJAjsU_n"
   },
   "outputs": [],
   "source": [
    "url_col = 'url' # url is used as IDs\n",
    "prediction_col = 'housing'\n",
    "output_dir = \"/content/output\"\n",
    "\n",
    "# WHAT WILL END UP IN OUTPUT.CSV\n",
    "texts_test = df_test[\"headline\"].tolist()   # FEATURES  (X)\n",
    "urls = df_test[url_col].tolist()            # ID\n",
    "housing_test = df_test[\"housing\"].tolist()  # TARGET    (Y)\n",
    "\n",
    "# ensure that NOTHING IS NONETYPE. this still happens here\n",
    "# and its inconvenient, and its frustrating, but its not impossible to fix üòÆ‚Äçüí®\n",
    "offaxis_removal = 0\n",
    "for i in range(len(texts_test)):\n",
    "  if not(isinstance(texts_test[i-offaxis_removal], str)):\n",
    "    del texts_test[i-offaxis_removal] # FEATURES\n",
    "    del urls[i-offaxis_removal] # IDS \n",
    "    del housing_test[i-offaxis_removal] # TARGETS\n",
    "    offaxis_removal += 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bLRr3mgeTGO2",
   "metadata": {
    "id": "bLRr3mgeTGO2"
   },
   "source": [
    "## Prepare Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3P3Z2rrTJzq",
   "metadata": {
    "id": "f3P3Z2rrTJzq"
   },
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"ari_aap_logs\")\n",
    "def get_run_logdir(model): # use a new directory for each run\n",
    "  run_id = datetime.now(pytz.timezone(\"Asia/Singapore\")).strftime(model+'_run_%Y-%m-%d_%H-%M-%S_SGT')\n",
    "  return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir_sentencepiece = get_run_logdir(\"sentencepiece\")\n",
    "run_logdir_word = get_run_logdir(\"word\")\n",
    "\n",
    "tensorboard_cb_sentencepiece = TensorBoard(run_logdir_sentencepiece)\n",
    "tensorboard_cb_word = TensorBoard(run_logdir_word)\n",
    "\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#   filepath=\"bestcheckpoint\",\n",
    "#   save_weights_only=True,\n",
    "#   monitor='val_accuracy',\n",
    "#   mode='max',\n",
    "#   save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SWopCIrj0qg4",
   "metadata": {
    "id": "SWopCIrj0qg4"
   },
   "source": [
    "## <em>Subword-level</em> tokenization\n",
    "Using `sentencepiece`, a third-party Python library, we will encode the headlines into subwords, and put it into a subword vocab file. The following are what each cell does until the next markdown cell\n",
    "- Create `train.txt` which is basically where all `headlines` will be temporarily stored for the next cell.\n",
    "- Utilize `sentencepiece` for tokenization into subwords\n",
    "- The 4th cell from here is just the steps to convert and encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6-bstdhb0p0U",
   "metadata": {
    "id": "6-bstdhb0p0U"
   },
   "outputs": [],
   "source": [
    "temp_file = 'train.txt'\n",
    "with open(temp_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(texts)) \n",
    "    # if this says NoneType error btw, may as well defenestrate yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DHnDAv5r1Z35",
   "metadata": {
    "id": "DHnDAv5r1Z35"
   },
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
    "\n",
    "max_num_words = 12000\n",
    "model_type = 'bpe' # 'unigram' is default, i would go with bpe cuz subwords\n",
    "model_prefix = model_type\n",
    "pad_id = 0 # padding to make samples same size\n",
    "unk_id = 1 # unknown words\n",
    "bos_id = 2 # beginning of sentence\n",
    "eos_id = 3 # end of sentence\n",
    "\n",
    "def rpjmnw(max_num_words_infunct):\n",
    "  return ' '.join(['--input={}'.format(temp_file),'--model_type={}'.format(model_type),'--model_prefix={}'.format(model_type),'--vocab_size={}'.format(max_num_words_infunct),'--pad_id={}'.format(pad_id),'--unk_id={}'.format(unk_id),'--bos_id={}'.format(bos_id),'--eos_id={}'.format(eos_id)])\n",
    "\n",
    "print(rpjmnw(max_num_words))\n",
    "SentencePieceTrainer.train(rpjmnw(max_num_words))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uPP4gNRz1fj3",
   "metadata": {
    "id": "uPP4gNRz1fj3"
   },
   "outputs": [],
   "source": [
    "# create SentencePieceProcessor. Tokenizes a string to subwords\n",
    "sp = SentencePieceProcessor()\n",
    "sp.load(\"{}.model\".format(model_prefix))\n",
    "print('Found %s unique tokens.' % sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F3B-0KNK1jx5",
   "metadata": {
    "id": "F3B-0KNK1jx5"
   },
   "outputs": [],
   "source": [
    "# this var is what you know about the data\n",
    "# since headlines aren't as long as movie reviews, I set it as 16 for now\n",
    "# also adjust the model when adjusting this too\n",
    "max_sequence_len = 16\n",
    "\n",
    "# line below will tokenize (convert words to numbers)\n",
    "sequences_train1 = [sp.encode_as_ids(str(text)) for text in texts_train]\n",
    "# pads tokenized sequence to make it compatible with model\n",
    "x_train1 = pad_sequences(sequences_train1, maxlen=max_sequence_len)\n",
    "\n",
    "# line below will tokenize (convert words to numbers)\n",
    "sequences_val1 = [sp.encode_as_ids(str(text)) for text in texts_val]\n",
    "# pads tokenized sequence to make it compatible with model\n",
    "x_val1 = pad_sequences(sequences_val1, maxlen=max_sequence_len)\n",
    "\n",
    "\n",
    "print(sequences_train1[0][:5])\n",
    "print(x_train1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dXv27PY1khQ",
   "metadata": {
    "id": "4dXv27PY1khQ"
   },
   "outputs": [],
   "source": [
    "print('sample text: ', texts_train[0])\n",
    "print('sample text: ', sp.encode_as_pieces(sp.decode_ids(x_train1[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j4nPdC1w1o8S",
   "metadata": {
    "id": "j4nPdC1w1o8S"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "model1 = text_cnn(max_sequence_len, max_num_words + 1, num_classes)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ELPhUrhTM9f",
   "metadata": {
    "id": "6ELPhUrhTM9f"
   },
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UuiNoIkL1srJ",
   "metadata": {
    "id": "UuiNoIkL1srJ"
   },
   "outputs": [],
   "source": [
    "# TRAINING HERE\n",
    "start = time.time()\n",
    "history1 = model1.fit(x_train1, y_train,\n",
    "                      validation_data=(x_val1, y_val),\n",
    "                      batch_size=16, \n",
    "                      epochs=8,\n",
    "                      callbacks=[tensorboard_cb_sentencepiece])#, model_checkpoint_callback])\n",
    "print(\"\\nTime taken to train subword-level model: \"+str(timedelta(seconds=time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IVeltmVSz2KM",
   "metadata": {
    "id": "IVeltmVSz2KM"
   },
   "outputs": [],
   "source": [
    "# check out more details after training\n",
    "\n",
    "plt.plot(model1.history.history['acc'])\n",
    "plt.plot(model1.history.history['val_acc'])\n",
    "plt.title('subword-token model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model1.history.history['loss'])\n",
    "plt.plot(model1.history.history['val_loss'])\n",
    "plt.title('subword-token model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zk5rLwhWJKYf",
   "metadata": {
    "id": "zk5rLwhWJKYf"
   },
   "source": [
    "## <em>Word-level</em> tokenization\n",
    "Do something like what i did on for subword-level, but instead using word-level tokenization. Combining the use of both subword-level tokenization & word-level tokenization will ensure meaning behind the words are known correctly. More details can be found here -> https://aclanthology.org/2020.acl-srw.10/\n",
    "\n",
    "However, all i'm doing for now is getting which model is better at classifying ST articles as housing or not.\n",
    "\n",
    "<br><br>\n",
    "btw both model trainings uses a huge batch size and 1 epoch because the web-scraping resulted in actual Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eOMKVF7J3jJ",
   "metadata": {
    "id": "3eOMKVF7J3jJ"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hL2RMXEkKPW_",
   "metadata": {
    "id": "hL2RMXEkKPW_"
   },
   "outputs": [],
   "source": [
    "# line below will tokenize (convert words to numbers)\n",
    "sequences_train2 = tokenizer.texts_to_sequences(texts_train)\n",
    "# pads tokenized sequence to make it compatible with model\n",
    "x_train2 = pad_sequences(sequences_train2, maxlen=max_sequence_len)\n",
    "\n",
    "# line below will tokenize (convert words to numbers)\n",
    "sequences_val2 = tokenizer.texts_to_sequences(texts_val)\n",
    "# pads tokenized sequence to make it compatible with model\n",
    "x_val2 = pad_sequences(sequences_val2, maxlen=max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VaNCs7tdKRq2",
   "metadata": {
    "id": "VaNCs7tdKRq2"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "model2 = text_cnn(max_sequence_len, max_num_words + 1, num_classes)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JqkB43eVSFk",
   "metadata": {
    "id": "1JqkB43eVSFk"
   },
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lRQbIr_BKU1b",
   "metadata": {
    "id": "lRQbIr_BKU1b"
   },
   "outputs": [],
   "source": [
    "# TRAINING HERE\n",
    "start = time.time()\n",
    "history2 = model2.fit(x_train2, y_train,\n",
    "                      validation_data=(x_val2, y_val),\n",
    "                      batch_size=16, \n",
    "                      epochs=8,\n",
    "                      callbacks=[tensorboard_cb_word])#, model_checkpoint_callback])\n",
    "print(\"\\nTime taken to train word-level model: \"+str(timedelta(seconds=time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nUBw-h0N0m45",
   "metadata": {
    "id": "nUBw-h0N0m45"
   },
   "outputs": [],
   "source": [
    "# check out more details after training\n",
    "\n",
    "plt.plot(model2.history.history['acc'])\n",
    "plt.plot(model2.history.history['val_acc'])\n",
    "plt.title('word-token model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(model2.history.history['loss'])\n",
    "plt.plot(model2.history.history['val_loss'])\n",
    "plt.title('word-token model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NkwfOx6LVK87",
   "metadata": {
    "id": "NkwfOx6LVK87"
   },
   "source": [
    "# Initiate Testing\n",
    "\n",
    "## Create test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QbMTOKpeVKHT",
   "metadata": {
    "id": "QbMTOKpeVKHT"
   },
   "outputs": [],
   "source": [
    "# word-level test sequences\n",
    "# line below will tokenize (convert words to numbers)\n",
    "word_sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "# pads tokenized sequence to make it compatible with model\n",
    "word_x_test = pad_sequences(word_sequences_test, maxlen=max_sequence_len)\n",
    "print(\"Word-level x_test: \"+str(len(word_x_test)))\n",
    "\n",
    "# subword-level test sequences\n",
    "# line below will tokenize (convert words to numbers)\n",
    "sentencepiece_sequences_test = [sp.encode_as_ids(str(text)) for text in texts_test]\n",
    "# pads tokenized sequence to make it compatible with model\n",
    "sentencepiece_x_test = pad_sequences(sentencepiece_sequences_test, maxlen=max_sequence_len)\n",
    "print(\"Subword-level x_test: \"+str(len(sentencepiece_x_test)))\n",
    "\n",
    "# both should be of same length, else there's something horribly horribly wrong "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50QkqdVTr1",
   "metadata": {
    "id": "bf50QkqdVTr1"
   },
   "source": [
    "### Output function for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HIBkITvkL6I5",
   "metadata": {
    "id": "HIBkITvkL6I5"
   },
   "outputs": [],
   "source": [
    "# ease things up using functions\n",
    "def create_output(urls, predictions, url_col, prediction_col, output_path) -> pd.DataFrame:\n",
    "    df_output = pd.DataFrame({\n",
    "        url_col: urls,\n",
    "        prediction_col: predictions.round(6) * 100\n",
    "    }, columns=[url_col, prediction_col])\n",
    "\n",
    "    ## NOTE: Predictions will come out as percentage, not by 1\n",
    "\n",
    "    if output_path is not None:\n",
    "        # create the directory if need be, e.g. if the output_path = output/output.csv\n",
    "        # we'll create the output directory first if it doesn't exist\n",
    "        directory = os.path.split(output_path)[0]\n",
    "        if (directory != '' or directory != '.') and not os.path.isdir(directory):\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    df_output.to_csv(output_path, index=False, header=True)\n",
    "\n",
    "    return df_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yj_LjO6WVZ7a",
   "metadata": {
    "id": "yj_LjO6WVZ7a"
   },
   "source": [
    "## Predictions\n",
    "This is where the bulk of prediciton would go to, won't take more than 10min. \n",
    "\n",
    "Note the first line, the `predictions_dict` variable declaration.\n",
    "  * note the `[:, 1]`, its because of how the output's formed\n",
    "  * `[1. 0.]` & `[0. 1.]` representing 0 & 1 respectively\n",
    "  * `:` means all the rows, `, 1` means looking at the second value ^ only\n",
    "\n",
    "Next line generates output to CSVs for accuracy comparison later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C30cBZjDssG_",
   "metadata": {
    "id": "C30cBZjDssG_"
   },
   "outputs": [],
   "source": [
    "predictions_dict = {\n",
    "    'sentencepiece_cnn': model1.predict(sentencepiece_x_test)[:, 1], \n",
    "    'word_cnn': model2.predict(word_x_test)[:, 1] \n",
    "}\n",
    "\n",
    "# right after prediction, write outputs to CSVs based on their models\n",
    "for model_name, predictions in predictions_dict.items():\n",
    "    print('generating output for: ', model_name)\n",
    "    output_path = '/content/{}_output.csv'.format(model_name)\n",
    "    df_output = create_output(urls, predictions, url_col, prediction_col, output_path)\n",
    "\n",
    "# sanity check to make sure the size and the output of the output makes sense\n",
    "print(df_output.shape)\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "che8UVHH815W",
   "metadata": {
    "id": "che8UVHH815W"
   },
   "source": [
    "## üë©üèæ‚Äçüíª Evaluate & launch TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jckXjkKl836q",
   "metadata": {
    "id": "jckXjkKl836q"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "\n",
    "print(\"Evaluate on test data\\n\") # predicted results, truth \n",
    "results1 = model1.evaluate(word_x_test, np.array(housing_test).astype('float32').reshape((-1,1)), batch_size=512)\n",
    "print(\"SentencePiece:\\n\\ttest loss, test acc: \"+ str(results1) +\"\\n\")\n",
    "results2 = model2.evaluate(sentencepiece_x_test, np.array(housing_test).astype('float32').reshape((-1,1)), batch_size=512)\n",
    "print(\"Word:\\n\\ttest loss, test acc: \"+ str(results2)+\"\\n\")\n",
    "print(\"\\nThe tokenization method that performed better is: \"+str(\"Sub-word tokenization\" if results1[1] > results2[1] else \"Word tokenization\"), end=\"\\n\\n\\n\\n\")\n",
    "\n",
    "%tensorboard --logdir /content/ari_aap_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-tyRQmRIXscF",
   "metadata": {
    "id": "-tyRQmRIXscF"
   },
   "source": [
    "Everything below this line is not relevant to the project, rather its for convenience.\n",
    "\n",
    "---\n",
    "\n",
    "# Save it all\n",
    "Saves the models, the scraped data & the outputs from each model.\n",
    "\n",
    "Oh and also check how long it took to run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pDwd9wKWTBtU",
   "metadata": {
    "id": "pDwd9wKWTBtU"
   },
   "outputs": [],
   "source": [
    "model1.save(\"/content/sentencepiece_model\")\n",
    "model2.save(\"/content/word_model\")\n",
    "notebookname = \"AAP_shortlist-articles\"\n",
    "%notebook AAP_shortlist-articles_history_no-markdown.ipynb\n",
    "currentdatetime = datetime.now(pytz.timezone(\"Asia/Singapore\")).strftime('%Y-%m-%d_%H-%M-%S_SGT')\n",
    "outputzip = \"[\"+currentdatetime+\"]_\"+notebookname+\".zip\"\n",
    "os.system(\"zip -r \"+outputzip+\" /content/* -x /content/sample_data/\\* /content/.config/\\* /content/.ipynb_checkpoints/\\*\")\n",
    "\n",
    "%cd /content\n",
    "\n",
    "# your details here to make a zipped copy of this notebook project\n",
    "username = \"\"\n",
    "email = \"\"\n",
    "ghp_key = \"\"\n",
    "colab_output_repo = \"colab-outputs\"\n",
    "\n",
    "os.system(\"git config --global user.name \\\"\"+username+\"\\\"\")\n",
    "os.system(\"git config --global user.email \\\"\"+email+\"\\\"\")\n",
    "os.system(\"git clone --depth=1 https://\"+username+\":\"+ghp_key+\"@github.com/\"+username+\"/\"+colab_output_repo+\"\")\n",
    "os.system(\"mv /content/\"+outputzip+\" /content/\"+colab_output_repo+\"/\"+outputzip+\" && cd \"+colab_output_repo+\" && git add . && git commit -m \"+currentdatetime+\"_\"+notebookname+\" && git push\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4pcw9VTKUd24",
   "metadata": {
    "id": "4pcw9VTKUd24"
   },
   "outputs": [],
   "source": [
    "print(\"Total time taken to run this notebook: \"+str(timedelta(seconds=time.time() - time_alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NyCUjxbJVLWj",
   "metadata": {
    "id": "NyCUjxbJVLWj"
   },
   "outputs": [],
   "source": [
    "# wait for 30 seconds to ensure that everything is executed before google colab commit sepukku\n",
    "# time.sleep(30)\n",
    "\n",
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LbwZxfbZRt8x",
   "metadata": {
    "id": "LbwZxfbZRt8x"
   },
   "source": [
    "# Summary\n",
    "This written after the final training and milestone report.\n",
    "\n",
    "So there were many things that I have learnt throughout the creation of this notebook and before & after the project milestone report\n",
    "\n",
    "1. **The importance of getting balanced data**. The most greatest revelation was that I was training on very imbalanced data until about 2 days after the project milestone report. Initially, I had about _3000_ articles that are about housing, which is about **1.71%** of the _175000_ articles that I had in the training, and with 3000 articles I was still being generous in the keywords. It wasn't when I started considering simply cutting off some of the non-housing articles, because it felt like if I gave it a closer balance, the model would be able to identify housing articles better. Little did I know that it was a legit problem that it would cause the model to severely overfit. I figured that simply cutting off non-housing articles would be the best choice, as I had about _175000_ rows to deal with. Cutting off a digit or two from that number helped tremendously in training, and the model is able to finally give good predictions in the end. \n",
    "1. **The importance of knowing how to build a model**. Not much to say about this one, because its something that I had a lot trouble with when trying to use different layers and knowing what numbers do what. Using **model.summary()** after model.compile() is very helpful in understanding the existing model. It also absolutely helps to know about layers from a different source too.\n",
    "1. **The importance of using the best & most efficient methods of data collection**. Initially I used UiPath Studio to create a CSV file where it contained only 5000 rows of data. As versatile & powerful UiPath Studio is, I was interested in making everything available in this notebook itself instead of updating and uploading the CSV file once in a while. Not only was I able to increase my dataset to 175000 unique rows, I will also be able to constantly get updated results according to Straits Times' latest articles (that are uploaded to their main site).\n",
    "1. **The importance of experimentation**. This is essential in A.I. Science, its what makes A.I. science a science. I cross-referenced my subword-level tokenization and word-level tokenization processes after a blog online, and even though I did somewhat understand why it showed both types of tokenization, only after much painstaking training and modification on my own notebook to fit my purpose did it really hit me that creating A.I. is not easy. It requires experimentation, documentation, and a lot more. This whole experience really made me give my appreciation for trailblazers and masters of A.I., and admittedly, really encourages me to potentially pursue A.I. as a career. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
